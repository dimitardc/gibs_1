{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF files example reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIRECTORY = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA_DIRECTORY + \"testFile.hdf\", \"w\") as file:\n",
    "    dset = file.create_dataset(\"dataset\", (10, 10), dtype='i') \n",
    "    dset[0,0] = 2\n",
    "    print(\"HDF5 file created successfully.\")\n",
    "    dset.attrs['time_del']  = 0.1\n",
    "    dset.attrs['time_del2'] = 0.2\n",
    "    dset.attrs['time_del3'] = 0.3\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + \"testFile.hdf\") as file:\n",
    "    dset = file[\"dataset\"]\n",
    "    for attr_name, attr_value in dset.attrs.items():\n",
    "        print(f\"{attr_name}: {attr_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Saving and Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(1000)\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'w') as file:\n",
    "    dset = file.create_dataset(\"default\", data=arr)\n",
    "    dset2 = file.create_dataset(\"newSet\", (10,10), dtype='i')\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'r') as file:\n",
    "   data = file['default']     \n",
    "   print(min(data))\n",
    "   print(max(data))\n",
    "   print(data[:15])\n",
    "   print(file.keys())\n",
    "   print(type(data))\n",
    "\n",
    "file = h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'r')\n",
    "data = file['default'][()]\n",
    "file.close()\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Reading from HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmbr = 10000\n",
    "arr1 = np.random.randn(nmbr)\n",
    "arr2 = np.random.randn(nmbr)\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'w') as file:\n",
    "    file.create_dataset('array_1', data=arr1)\n",
    "    file.create_dataset('array_2', data=arr2)\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'r') as file:\n",
    "    d1 = file['array_1']\n",
    "    d2 = file['array_2']\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(d1)):\n",
    "        if d1[i] > 0:                       \n",
    "            data.append(d2[i])\n",
    "\n",
    "print('The length of data with a for loop: {}'.format(len(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Writing to HDF5 Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(100)\n",
    "\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'w') as file:\n",
    "   dset = file.create_dataset(\"default\", (1000))\n",
    "   dset[10:20] = arr[50:60]\n",
    "\n",
    "arr = np.random.randn(1000)\n",
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'w') as file:\n",
    "   dset = file.create_dataset(\"default\", (1000,))\n",
    "   dset[:] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA_DIRECTORY + 'testFile.hdf', 'w') as file:\n",
    "   dset_int_1 = file.create_dataset('integers', (10, ), dtype='i1')        #int of 1 byte\n",
    "   dset_int_8 = file.create_dataset('integers8', (10, ), dtype='i8')       #int of 8 byte\n",
    "   dset_complex = file.create_dataset('complex', (10, ), dtype='c16')      #complex num of 16 byte\n",
    "\n",
    "   dset_int_1[0] = 1200\n",
    "   dset_int_8[0] = 1200.1\n",
    "   dset_complex[0] = 3 + 4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successful data reading from a file downloaded from Earthdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA_DIRECTORY + 'earthdataFile.HDF5', 'r') as file:\n",
    "    print(file.keys())\n",
    "    group = file['S1']\n",
    "    print(type(group))\n",
    "    \n",
    "    print(\"Datasets within the group:\")\n",
    "    for dataset_name in group.keys():\n",
    "        print(dataset_name)\n",
    "\n",
    "    dataSet = group['probabilityOfPrecip']\n",
    "    print(type(dataSet))\n",
    "\n",
    "    data = dataSet[:]\n",
    "    print(type(data))\n",
    "\n",
    "    print(data[0,0])\n",
    "\n",
    "# group -> dataset -> data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
